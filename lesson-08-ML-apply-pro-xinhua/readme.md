## Assignmet: lesson-07&lesson-08的【小项目】文本相似性检测与抄袭判断



## ML各个模型优缺点分析

### LR线性回归
**优点:** 简单，速度快，有解释性 

**缺点:** 对复杂的数据集无法很好的拟合，对共线性敏感



### LC逻辑回归
**优点:** 
1. 预测结果是界于0和1之间的概率； 
2. 可以适用于连续性和类别性自变量； 
3. 容易使用和解释；

**缺点:**
1. 对模型中自变量多重共线性较为敏感，
例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。
需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性； 

2. 预测结果呈“S”型，因此从log(odds)向概率转化的过程是非线性的，在两端随着log(odds)值的变化，
概率变化很小，边际值太小，slope太小，而中间概率的变化很大，很敏感。 导致很多区间的变量变化对目标概率的影响没有区分度，无法确定阀值。



###  KNN算法
**优点:** 

1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归； 
2. 可用于非线性分类； 
3. 训练时间复杂度为O(n)； 
4. 准确度高，对数据没有假设，对outlier不敏感；

**缺点:**
1. 计算量大； 
2. 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）； 
3. 需要大量的内存；



###  SVM

**优点:** 
较准确，但是解释性较差，处理非线性不错，可以解决非平衡问题（乘上不同的权重） 

**SVM有如下主要几个特点：**  
1. 非线性映射是SVM方法的理论基础,SVM利用内积核函数代替向高维空间的非线性映射； 
2. 对特征空间划分的最优超平面是SVM的目标,最大化分类边际的思想是SVM方法的核心； 
3. 支持向量是SVM的训练结果,在SVM分类决策中起决定作用的是支持向量； 
4. SVM 是一种有坚实理论基础的新颖的小样本学习方法。它基本上不涉及概率测度及大数定律等,因此不同于现有的统计方法。从本质上看,它避开了从归纳到演绎的传统过程,实现了高效的从训练样本到预报样本的“转导推理”,大大简化了通常的分类和回归等问题； 
5. SVM 的最终决策函数只由少数的支持向量所确定,计算的复杂性取决于支持向量的数目,而不是样本空间的维数,这在某种意义上避免了“维数灾难”。 
6. 少数支持向量决定了最终结果,这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单,而且具有较好的“鲁棒”性。

**缺点:**
1. SVM算法对大规模训练样本难以实施，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数） ，当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。 
2. 用SVM解决多分类问题存在困难,经典的支持向量机算法只给出了二类分类的算法，而在数据挖掘的实际应用中，一般要解决多类的分类问题



### NB-Naive-Bayes

**优点:** 理论简单，有解释性 
1. 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
2. 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。 
3. 对缺失数据不太敏感，算法也比较简单，常用于文本分类。

**缺点:**
1. 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的， 在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。 
2. 需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。 
3. 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。 
4. 对输入数据的表达形式很敏感。

### 决策树 
**优点:** 可以对特征重要度进行排序 决策树算法的优点： 
1. 理解和解释起来简单，且决策树模型可以想象 
2. 需要准备的数据量不大，而其他的技术往往需要很大的数据集，需要创建虚拟变量，去除不完整的数据，但是该算法对于丢失的数据不能进行准确的预测 
3. 决策树算法的时间复杂度(即预测数据)是用于训练决策树的数据点的对数 
4. 能够处理数字和数据的类别（需要做相应的转变），而其他算法分析的数据集往往是只有一种类型的变量 
5. 能够处理多输出的问题 （多分类问题）
6. 使用白盒模型，如果给定的情况是在一个模型中观察到的，该条件的解释很容易解释的布尔逻辑，相比之下，在一个黑盒子模型（例如人工神经网络），结果可能更难以解释 
7. 可能使用统计检验来验证模型，这是为了验证模型的可靠性
8. 从数据结果来看，它执行的效果很好，虽然它的假设有点违反真实模型

**缺点:**
1. 决策树算法学习者可以创建复杂的树，但是没有推广依据，这就是所谓的过拟合，
为了避免这种问题，出现了剪枝的概念，即设置一个叶子结点所需要的最小数目或者设置树的最大深度 
2. 决策树的结果可能是不稳定的，因为在数据中一个很小的变化可能导致生成一个完全不同的树，这个问题可以通过使用集成决策树来解决 
3. 众所周知，学习一恶搞最优决策树的问题是NP——得到几方面完全的优越性，甚至是一些简单的概念。
因此，实际决策树学习算法是基于启发式算法，如贪婪算法，寻求在每个节点上的局部最优决策。这样的算法不能保证返回全局最优决策树。
这可以减轻训练多棵树的合奏学习者，在那里的功能和样本随机抽样更换。 
4. 这里有一些概念是很难的理解的，因为决策树本身并不难很轻易的表达它们，比如说异或校验或复用的问题。 
5. 决策树学习者很可能在某些类占主导地位时创建有有偏异的树，因此建议用平衡的数据训练决策树

