{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": "{\n\"cells\": [\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"# 新华社新闻抄袭自动判别\\n\",\n\"任务要求：\\n\",\n\"1. 构建一个机器学习模型，判断这个文章是不是新华社的\\n\",\n\"2. 当这个模型的acc 大于 0.8778， recall， precision，f1等值都较高的时候\\n\",\n\"3. 用该模型 判断一篇文章是否是新华社的文章，如果判断出来是新华社的，但是，它的source并不是新华社的，那么，我们就说，这个文章是抄袭的新华社的文章\\n\",\n\"4. Text Representation uses \\\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\\\"\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"## 数据预处理\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"### 获取数据\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 3,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"import pandas as pd\\n\",\n\"import numpy as np\\n\",\n\"import math\\n\",\n\"import jieba\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 4,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"filename = 'E:\\\\\\\\MYGIT\\\\\\\\DataSources\\\\\\\\sqlResult_1558435.csv'\\n\",\n\"pandas_data = pd.read_csv(filename, encoding='gb18030')\\n\",\n\"pandas_data = pandas_data.dropna(subset = ['content']) #剔除内容为空的行\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 5,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"content = pandas_data['content'].tolist()\\n\",\n\"source = pandas_data['source'].tolist()\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 7,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"source = [name if isinstance(name, str) else 'unknow' for name in source] #把没有来源的信息标记为unknow\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"### content清洗切词，source分好标签\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 8,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"article_labels = [1 if name.strip() == '新华社' else 0 for name in source]\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 9,\n\"metadata\": {},\n\"outputs\": [\n{\n\"name\": \"stdout\",\n\"output_type\": \"stream\",\n\"text\": [\n\"78661 87054 87054\\n\"\n]\n}\n],\n\"source\": [\n\"print(sum(article_labels), len(article_labels),len(content))\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"以上可见正负样本非常不均衡，如何解决？\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 7,\n\"metadata\": {\n\"code_folding\": []\n},\n\"outputs\": [],\n\"source\": [\n\"import os\\n\",\n\"from pyltp import Segmentor\\n\",\n\"#获取停用词集\\n\",\n\"def get_stopwords():\\n\",\n\" stopwords = []\\n\",\n\" with open('stopwords.txt') as f:\\n\",\n\" line_str = f.readline()\\n\",\n\" while line_str!= '':\\n\",\n\" line_str = line_str.strip()\\n\",\n\" stopwords.append(line_str)\\n\",\n\" line_str = f.readline()\\n\",\n\" return set(stopwords)\\n\",\n\"\\n\",\n\"def text_deal_cut(text_list):\\n\",\n\" stopwords = get_stopwords()\\n\",\n\" cws_model_path = 'E:/MYGIT/Project/ltp_data/cws.model'\\n\",\n\" \\n\",\n\" segmentor = Segmentor() # 初始化实例\\n\",\n\" segmentor.load(cws_model_path) # 加载模型 \\n\",\n\" corpus = []\\n\",\n\" i = 0\\n\",\n\" for string in text_list:\\n\",\n\" i += 1\\n\",\n\" if(i000 == 0):print(i)\\n\",\n\" string = string.strip()\\n\",\n\" string_temp = ''\\n\",\n\" words = list(segmentor.segment(string))\\n\",\n\" for word in words:\\n\",\n\" if word not in stopwords:\\n\",\n\" string_temp += word + ' ' \\n\",\n\" corpus.append(string_temp)\\n\",\n\" segmentor.release() # 释放模型\\n\",\n\" return corpus\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 8,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/plain\": [\n\"87054\"\n]\n},\n\"execution_count\": 8,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"len(content)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 9,\n\"metadata\": {},\n\"outputs\": [\n{\n\"name\": \"stdout\",\n\"output_type\": \"stream\",\n\"text\": [\n\"3000\\n\",\n\"6000\\n\",\n\"9000\\n\",\n\"12000\\n\",\n\"15000\\n\",\n\"18000\\n\",\n\"21000\\n\",\n\"24000\\n\",\n\"27000\\n\",\n\"30000\\n\",\n\"33000\\n\",\n\"36000\\n\",\n\"39000\\n\",\n\"42000\\n\",\n\"45000\\n\",\n\"48000\\n\",\n\"51000\\n\",\n\"54000\\n\",\n\"57000\\n\",\n\"60000\\n\",\n\"63000\\n\",\n\"66000\\n\",\n\"69000\\n\",\n\"72000\\n\",\n\"75000\\n\",\n\"78000\\n\",\n\"81000\\n\",\n\"84000\\n\",\n\"87000\\n\"\n]\n}\n],\n\"source\": [\n\"corpus = text_deal_cut(content)\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"到目前为止，已经获得了处理好的标签，和分词好的文章\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 12,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"import pickle\\n\",\n\"with open('./temp_file/corpus_list','wb') as f:\\n\",\n\" pickle.dump(corpus,f)\\n\",\n\"with open('./temp_file/corpus_list_label','wb') as f:\\n\",\n\" pickle.dump(article_labels,f)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 1,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"#读取\\n\",\n\"import pickle\\n\",\n\"try:\\n\",\n\" print(corpus[0])\\n\",\n\" print(article_labels[:10])\\n\",\n\"except NameError:\\n\",\n\" with open('./temp_file/corpus_list','rb') as f:\\n\",\n\" corpus = pickle.load(f)\\n\",\n\" with open('./temp_file/corpus_list_label','rb') as f:\\n\",\n\" corpus_label = pickle.load(f)\\n\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 2,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/plain\": [\n\"[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\"\n]\n},\n\"execution_count\": 2,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"corpus_label[:10]\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 3,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/plain\": [\n\"'本周 6月 12日 小米 手机 15 款 机型 外 机型 暂停 更新 发布 含 开发版 体验版 内测 稳定 版 暂 受 影响 确保 工程师 精力 系统 优化 工作 有人 猜测 精力 主要 MIUI 研发 之中 MIUI 去年 5月 发布 年 有余 更新换代 MIUI 确切 信息 等待 官方 消息 '\"\n]\n},\n\"execution_count\": 3,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"corpus[0]\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 4,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"ss = corpus[0]\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 5,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/plain\": [\n\"'本周 6月 12日 小米 手机 15 款 机型 外 机型 暂停 更新 发布 含 开发版 体验版 内测 稳定 版 暂 受 影响 确保 工程师 精力 系统 优化 工作 有人 猜测 精力 主要 MIUI 研发 之中 MIUI 去年 5月 发布 年 有余 更新换代 MIUI 确切 信息 等待 官方 消息 '\"\n]\n},\n\"execution_count\": 5,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"ss.replace('\\\\r\\\\n','').replace('。','')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 6,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/plain\": [\n\"'本周 6月 12日 小米 手机 15 款 机型 外 机型 暂停 更新 发布 含 开发版 体验版 内测 稳定 版 暂 受 影响 确保 工程师 精力 系统 优化 工作 有人 猜测 精力 主要 MIUI 研发 之中 MIUI 去年 5月 发布 年 有余 更新换代 MIUI 确切 信息 等待 官方 消息 '\"\n]\n},\n\"execution_count\": 6,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"corpus = [line.replace('\\\\r\\\\n','').replace('。','') for line in corpus]\\n\",\n\"corpus[0]\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"## TFIDF向量化\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 7,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n\"import numpy as np\\n\",\n\"vectorized = TfidfVectorizer(max_features= 10000) #设置文本单词个数最大值\\n\",\n\"X = vectorized.fit_transform(corpus)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 21,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"vectorized = TfidfVectorizer(max_features= 5000) #设置文本单词个数最大值\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 22,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"sub_samples = corpus[:1000]\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 46,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"X = vectorized.fit_transform(corpus)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 47,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/plain\": [\n\"(87054, 10000)\"\n]\n},\n\"execution_count\": 47,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"X.shape\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"## 各类模型测试分析\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 2,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"import time\\n\",\n\"def clock(func):\\n\",\n\" def clocked(*args, **kwargs):\\n\",\n\" t0 = time.time()\\n\",\n\" result = func(*args, **kwargs)\\n\",\n\" elapsed = time.time() - t0\\n\",\n\" name = func.__name__\\n\",\n\" print('函数 {} 运行时间:{:.2f}s'.format(name,elapsed))\\n\",\n\" return result\\n\",\n\" return clocked\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"### 模型评估函数\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 3,\n\"metadata\": {\n\"code_folding\": []\n},\n\"outputs\": [],\n\"source\": [\n\"from sklearn.metrics import f1_score\\n\",\n\"from sklearn.metrics import accuracy_score\\n\",\n\"from sklearn.metrics import precision_score\\n\",\n\"from sklearn.metrics import recall_score\\n\",\n\"from sklearn.metrics import roc_auc_score\\n\",\n\"from sklearn.metrics import confusion_matrix\\n\",\n\"from sklearn.metrics import classification_report\\n\",\n\"import pandas as pd\\n\",\n\"\\n\",\n\"#@clock\\n\",\n\"\\n\",\n\"\\n\",\n\"def get_performance(clf, x_, y_):\\n\",\n\" y_hat = clf.predict(x_)\\n\",\n\" #result=pd.DataFrame({'F1':f1_score(y_, y_hat),\\n\",\n\" # 'ACC':accuracy_score(y_, y_hat),\\n\",\n\" # 'Precision':precision_score(y_, y_hat),\\n\",\n\" # 'Recall':recall_score(y_, y_hat)}) #不设置index\\n\",\n\" return [format(f1_score(y_, y_hat),'.4f'),format(accuracy_score(y_, y_hat),'.4f'),\\n\",\n\" format(f1_score(y_, y_hat),'.4f'),format(recall_score(y_, y_hat),'.4f')]\\n\",\n\" #print('f1_score is: {}'.format(f1_score(y_, y_hat)))\\n\",\n\" #print('accuracy is: {}'.format(accuracy_score(y_, y_hat)))\\n\",\n\" #print('percision is: {}'.format(precision_score(y_, y_hat)))\\n\",\n\" #print('recall is: {}'.format(recall_score(y_, y_hat)))\\n\",\n\" #print('roc_auc is: {}'.format(roc_auc_score(y_, y_hat)))\\n\",\n\" #print('confusion matrix: \\\\n{}'.format(confusion_matrix(y_, y_hat, labels=[0, 1])))\\n\",\n\" #print(classification_report(y_, y_hat))\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"### 数据预处理\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 4,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"from sklearn.model_selection import train_test_split\\n\",\n\"import random\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 5,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"corpus_0 = [a for a,i in zip(corpus, corpus_label) if i==0]\\n\",\n\"corpus_1 = [a for a,i in zip(corpus, corpus_label) if i==1]\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"均衡样本,这里把类别为1的砍掉一部分,只为调试试验\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 6,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"X = corpus_0 + random.sample(corpus_1, len(corpus_0))\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 7,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"y = [0]*len(corpus_0) + [1]*len(corpus_0)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 8,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n\"import numpy as np\\n\",\n\"vectorized = TfidfVectorizer(max_features= 5000) #设置文本单词个数最大值\\n\",\n\"X = vectorized.fit_transform(X)\\n\",\n\"X = X.toarray()\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 9,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"#选取30%左右数据作为测试数据\\n\",\n\"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 10,\n\"metadata\": {},\n\"outputs\": [\n{\n\"name\": \"stdout\",\n\"output_type\": \"stream\",\n\"text\": [\n\"5847 11750\\n\"\n]\n}\n],\n\"source\": [\n\"print(sum(y_train),len(y_train))\"\n]\n},\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n\"source\": [\n\"### 各类模型建立\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 28,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"from sklearn.neighbors import KNeighborsClassifier\\n\",\n\"from sklearn.linear_model import LogisticRegression\\n\",\n\"from sklearn.tree import DecisionTreeClassifier\\n\",\n\"from sklearn.naive_bayes import GaussianNB\\n\",\n\"from sklearn.naive_bayes import MultinomialNB\\n\",\n\"from sklearn.naive_bayes import ComplementNB\\n\",\n\"from sklearn.naive_bayes import BernoulliNB\\n\",\n\"from sklearn.svm import SVC\\n\",\n\"from sklearn.svm import NuSVC\\n\",\n\"from sklearn.svm import LinearSVC\\n\",\n\"from sklearn.ensemble import RandomForestClassifier\\n\",\n\"from sklearn.model_selection import GridSearchCV\\n\",\n\"import pandas as pd\\n\",\n\"import numpy as np\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 26,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"from sqlalchemy import create_engine\\n\",\n\"engine = create_engine(\\\"mysql://root:forever@127.0.0.1:3306/sklearn_result\\\", echo=False)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 41,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"def KNN(X_train, X_test, y_train, y_test,metric='minkowski'):\\n\",\n\" nbrs = KNeighborsClassifier(n_neighbors=5, algorithm='brute',metric =metric)\\n\",\n\" nbrs.fit(X_train, y_train)\\n\",\n\" return get_performance(nbrs,X_test,y_test)\\n\",\n\"\\n\",\n\"def logical_regression(X_train, X_test, y_train, y_test, solver='liblinear',C=1, penalty='l2', l1_ratio=None):\\n\",\n\" clf = LogisticRegression(solver=solver, C=C, penalty=penalty,l1_ratio=l1_ratio)\\n\",\n\" clf.fit(X_train, y_train)\\n\",\n\" return get_performance(clf,X_test,y_test)\\n\",\n\"\\n\",\n\"def decision_tree(X_train, X_test, y_train, y_test,criterion = 'gini',max_depth=100):\\n\",\n\" clf = DecisionTreeClassifier(criterion = criterion,max_depth= max_depth)\\n\",\n\" clf.fit(X_train, y_train)\\n\",\n\" return get_performance(clf,X_test,y_test) \\n\",\n\"\\n\",\n\"def Navie_baye(X_train, X_test, y_train, y_test, norm = False, method=1):\\n\",\n\" #method 只能是 0,1,2,3\\n\",\n\" method_list = [SVC, MultinomialNB,ComplementNB,BernoulliNB]\\n\",\n\" if method == 2:\\n\",\n\" clf = method_list[method](norm = norm)\\n\",\n\" clf.fit(X_train, y_train)\\n\",\n\" else:\\n\",\n\" clf = method_list[method]()\\n\",\n\" clf.fit(X_train, y_train)\\n\",\n\" return get_performance(clf,X_test,y_test) \\n\",\n\"\\n\",\n\"#SVM核函数的选择和参数比较重要，不合适的选择可能导致一直无法找到解，或求解时间过长\\n\",\n\"def svm_svc(X_train, X_test, y_train, y_test, C=1, kernel='linear', gamma='auto'):\\n\",\n\" clf = SVC(C=C, kernel=kernel, gamma= gamma,cache_size=1000.0,max_iter=300)\\n\",\n\" clf.fit(X_train, y_train)\\n\",\n\" return get_performance(clf,X_test,y_test) \\n\",\n\" \\n\",\n\"def svm_nusvc(X_train, X_test, y_train, y_test, nu=0.5, kernel='linear', gamma='auto'):\\n\",\n\" clf = NuSVC(nu=nu, kernel=kernel, gamma= gamma,cache_size=1000.0,max_iter=300)\\n\",\n\" clf.fit(X_train, y_train)\\n\",\n\" return get_performance(clf,X_test,y_test) \\n\",\n\"\\n\",\n\"def svm_linearsvc(X_train, X_test, y_train, y_test, penalty='l2',loss='squared_hinge', C=1):\\n\",\n\" clf = LinearSVC(penalty=penalty,loss=loss, C=C)\\n\",\n\" clf.fit(X_train, y_train)\\n\",\n\" return get_performance(clf,X_test,y_test) \\n\",\n\"\\n\",\n\"def random_forest(X_train, X_test, y_train, y_test,n_estimators=10,criterion='gini',max_depth=None):\\n\",\n\" clf = RandomForestClassifier(n_estimators=n_estimators,criterion=criterion,max_depth=max_depth)\\n\",\n\" clf.fit(X_train, y_train)\\n\",\n\" return get_performance(clf,X_test,y_test)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 53,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"#存放结果\\n\",\n\"array_0 = np.zeros((1,5))\\n\",\n\"colums = ['Method','F1', 'ACC','Precision','Recall']\\n\",\n\"result = pd.DataFrame(array_0, index=[0], columns=colums,dtype='float32')\\n\",\n\"for n_estimators in [10, 15, 20]: \\n\",\n\" for criterion in ['gini', 'entropy']: #['rbf']:#\\n\",\n\" for max_depth in [80, 120, 160]: \\n\",\n\" new_result = random_forest(X_train, X_test, y_train, y_test,\\n\",\n\" n_estimators=n_estimators,criterion=criterion,max_depth=max_depth)\\n\",\n\" result.loc[len(result)] = ['n:'+str(n_estimators)+'-'+\\n\",\n\" criterion+ '-depth:'+str(max_depth)]+new_result\\n\",\n\"#result.to_sql('de_tree_0', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 58,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/html\": [\n\"<div>\\n\",\n\"<style scoped>\\n\",\n\" .dataframe tbody tr th:only-of-type {\\n\",\n\" vertical-align: middle;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe tbody tr th {\\n\",\n\" vertical-align: top;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe thead th {\\n\",\n\" text-align: right;\\n\",\n\" }\\n\",\n\"</style>\\n\",\n\"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n\" <thead>\\n\",\n\" <tr style=\\\"text-align: right;\\\">\\n\",\n\" <th></th>\\n\",\n\" <th>Method</th>\\n\",\n\" <th>F1</th>\\n\",\n\" <th>ACC</th>\\n\",\n\" <th>Precision</th>\\n\",\n\" <th>Recall</th>\\n\",\n\" </tr>\\n\",\n\" </thead>\\n\",\n\" <tbody>\\n\",\n\" <tr>\\n\",\n\" <th>8</th>\\n\",\n\" <td>n:15-gini-depth:120</td>\\n\",\n\" <td>0.9828</td>\\n\",\n\" <td>0.9827</td>\\n\",\n\" <td>0.9828</td>\\n\",\n\" <td>0.9772</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>15</th>\\n\",\n\" <td>n:20-gini-depth:160</td>\\n\",\n\" <td>0.9800</td>\\n\",\n\" <td>0.9799</td>\\n\",\n\" <td>0.9800</td>\\n\",\n\" <td>0.9705</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>18</th>\\n\",\n\" <td>n:20-entropy-depth:160</td>\\n\",\n\" <td>0.9779</td>\\n\",\n\" <td>0.9780</td>\\n\",\n\" <td>0.9779</td>\\n\",\n\" <td>0.9654</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>12</th>\\n\",\n\" <td>n:15-entropy-depth:160</td>\\n\",\n\" <td>0.9774</td>\\n\",\n\" <td>0.9774</td>\\n\",\n\" <td>0.9774</td>\\n\",\n\" <td>0.9698</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>16</th>\\n\",\n\" <td>n:20-entropy-depth:80</td>\\n\",\n\" <td>0.9769</td>\\n\",\n\" <td>0.9770</td>\\n\",\n\" <td>0.9769</td>\\n\",\n\" <td>0.9650</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>14</th>\\n\",\n\" <td>n:20-gini-depth:120</td>\\n\",\n\" <td>0.9768</td>\\n\",\n\" <td>0.9768</td>\\n\",\n\" <td>0.9768</td>\\n\",\n\" <td>0.9662</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>10</th>\\n\",\n\" <td>n:15-entropy-depth:80</td>\\n\",\n\" <td>0.9766</td>\\n\",\n\" <td>0.9766</td>\\n\",\n\" <td>0.9766</td>\\n\",\n\" <td>0.9674</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>13</th>\\n\",\n\" <td>n:20-gini-depth:80</td>\\n\",\n\" <td>0.9765</td>\\n\",\n\" <td>0.9766</td>\\n\",\n\" <td>0.9765</td>\\n\",\n\" <td>0.9643</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>1</th>\\n\",\n\" <td>n:10-gini-depth:80</td>\\n\",\n\" <td>0.9764</td>\\n\",\n\" <td>0.9762</td>\\n\",\n\" <td>0.9764</td>\\n\",\n\" <td>0.9729</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>7</th>\\n\",\n\" <td>n:15-gini-depth:80</td>\\n\",\n\" <td>0.9760</td>\\n\",\n\" <td>0.9760</td>\\n\",\n\" <td>0.9760</td>\\n\",\n\" <td>0.9670</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>9</th>\\n\",\n\" <td>n:15-gini-depth:160</td>\\n\",\n\" <td>0.9756</td>\\n\",\n\" <td>0.9756</td>\\n\",\n\" <td>0.9756</td>\\n\",\n\" <td>0.9654</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>17</th>\\n\",\n\" <td>n:20-entropy-depth:120</td>\\n\",\n\" <td>0.9751</td>\\n\",\n\" <td>0.9752</td>\\n\",\n\" <td>0.9751</td>\\n\",\n\" <td>0.9631</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>11</th>\\n\",\n\" <td>n:15-entropy-depth:120</td>\\n\",\n\" <td>0.9745</td>\\n\",\n\" <td>0.9746</td>\\n\",\n\" <td>0.9745</td>\\n\",\n\" <td>0.9623</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>4</th>\\n\",\n\" <td>n:10-entropy-depth:80</td>\\n\",\n\" <td>0.9706</td>\\n\",\n\" <td>0.9708</td>\\n\",\n\" <td>0.9706</td>\\n\",\n\" <td>0.9533</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>6</th>\\n\",\n\" <td>n:10-entropy-depth:160</td>\\n\",\n\" <td>0.9695</td>\\n\",\n\" <td>0.9696</td>\\n\",\n\" <td>0.9695</td>\\n\",\n\" <td>0.9564</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>2</th>\\n\",\n\" <td>n:10-gini-depth:120</td>\\n\",\n\" <td>0.9691</td>\\n\",\n\" <td>0.9694</td>\\n\",\n\" <td>0.9691</td>\\n\",\n\" <td>0.9485</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>3</th>\\n\",\n\" <td>n:10-gini-depth:160</td>\\n\",\n\" <td>0.9652</td>\\n\",\n\" <td>0.9654</td>\\n\",\n\" <td>0.9652</td>\\n\",\n\" <td>0.9482</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>5</th>\\n\",\n\" <td>n:10-entropy-depth:120</td>\\n\",\n\" <td>0.9620</td>\\n\",\n\" <td>0.9625</td>\\n\",\n\" <td>0.9620</td>\\n\",\n\" <td>0.9387</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>0</th>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0.0000</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" </tr>\\n\",\n\" </tbody>\\n\",\n\"</table>\\n\",\n\"</div>\"\n],\n\"text/plain\": [\n\" Method F1 ACC Precision Recall\\n\",\n\"8 n:15-gini-depth:120 0.9828 0.9827 0.9828 0.9772\\n\",\n\"15 n:20-gini-depth:160 0.9800 0.9799 0.9800 0.9705\\n\",\n\"18 n:20-entropy-depth:160 0.9779 0.9780 0.9779 0.9654\\n\",\n\"12 n:15-entropy-depth:160 0.9774 0.9774 0.9774 0.9698\\n\",\n\"16 n:20-entropy-depth:80 0.9769 0.9770 0.9769 0.9650\\n\",\n\"14 n:20-gini-depth:120 0.9768 0.9768 0.9768 0.9662\\n\",\n\"10 n:15-entropy-depth:80 0.9766 0.9766 0.9766 0.9674\\n\",\n\"13 n:20-gini-depth:80 0.9765 0.9766 0.9765 0.9643\\n\",\n\"1 n:10-gini-depth:80 0.9764 0.9762 0.9764 0.9729\\n\",\n\"7 n:15-gini-depth:80 0.9760 0.9760 0.9760 0.9670\\n\",\n\"9 n:15-gini-depth:160 0.9756 0.9756 0.9756 0.9654\\n\",\n\"17 n:20-entropy-depth:120 0.9751 0.9752 0.9751 0.9631\\n\",\n\"11 n:15-entropy-depth:120 0.9745 0.9746 0.9745 0.9623\\n\",\n\"4 n:10-entropy-depth:80 0.9706 0.9708 0.9706 0.9533\\n\",\n\"6 n:10-entropy-depth:160 0.9695 0.9696 0.9695 0.9564\\n\",\n\"2 n:10-gini-depth:120 0.9691 0.9694 0.9691 0.9485\\n\",\n\"3 n:10-gini-depth:160 0.9652 0.9654 0.9652 0.9482\\n\",\n\"5 n:10-entropy-depth:120 0.9620 0.9625 0.9620 0.9387\\n\",\n\"0 0 0 0.0000 0 0\"\n]\n},\n\"execution_count\": 58,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"result['ACC']=result['ACC'].astype('float')\\n\",\n\"result.sort_values('ACC',ascending=False)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 55,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"result.to_sql('forest_0', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 15,\n\"metadata\": {},\n\"outputs\": [\n{\n\"name\": \"stderr\",\n\"output_type\": \"stream\",\n\"text\": [\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\",\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\svm\\\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=300). Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n\" % self.max_iter, ConvergenceWarning)\\n\"\n]\n}\n],\n\"source\": [\n\"#存放结果\\n\",\n\"array_0 = np.zeros((1,5))\\n\",\n\"colums = ['Method','F1', 'ACC','Precision','Recall']\\n\",\n\"result = pd.DataFrame(array_0, index=[0], columns=colums,dtype='float32')\\n\",\n\"for C in [0.5, 1, 5]: \\n\",\n\" for kernel in ['linear', 'poly', 'rbf', 'sigmoid']: #['rbf']:#\\n\",\n\" for gamma in ['auto', 'scale']: \\n\",\n\" new_result = svm_svc(X_train, X_test, y_train, y_test,C=C, kernel=kernel, gamma= gamma)\\n\",\n\" result.loc[len(result)] = ['C:'+str(C)+'-'+ kernel+'-'+ gamma]+new_result\\n\",\n\"#result.to_sql('de_tree_0', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 27,\n\"metadata\": {},\n\"outputs\": [\n{\n\"name\": \"stderr\",\n\"output_type\": \"stream\",\n\"text\": [\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\sql.py:1336: UserWarning: The provided table name 'SVM_0' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\\n\",\n\" warnings.warn(msg, UserWarning)\\n\"\n]\n}\n],\n\"source\": [\n\"result.to_sql('SVM_0', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 23,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"result['ACC']=result['ACC'].astype('float')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 25,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/html\": [\n\"<div>\\n\",\n\"<style scoped>\\n\",\n\" .dataframe tbody tr th:only-of-type {\\n\",\n\" vertical-align: middle;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe tbody tr th {\\n\",\n\" vertical-align: top;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe thead th {\\n\",\n\" text-align: right;\\n\",\n\" }\\n\",\n\"</style>\\n\",\n\"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n\" <thead>\\n\",\n\" <tr style=\\\"text-align: right;\\\">\\n\",\n\" <th></th>\\n\",\n\" <th>Method</th>\\n\",\n\" <th>F1</th>\\n\",\n\" <th>ACC</th>\\n\",\n\" <th>Precision</th>\\n\",\n\" <th>Recall</th>\\n\",\n\" </tr>\\n\",\n\" </thead>\\n\",\n\" <tbody>\\n\",\n\" <tr>\\n\",\n\" <th>18</th>\\n\",\n\" <td>C:5-linear-scale</td>\\n\",\n\" <td>0.9351</td>\\n\",\n\" <td>0.9335</td>\\n\",\n\" <td>0.9351</td>\\n\",\n\" <td>0.9485</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>17</th>\\n\",\n\" <td>C:5-linear-auto</td>\\n\",\n\" <td>0.9351</td>\\n\",\n\" <td>0.9335</td>\\n\",\n\" <td>0.9351</td>\\n\",\n\" <td>0.9485</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>10</th>\\n\",\n\" <td>C:1-linear-scale</td>\\n\",\n\" <td>0.9338</td>\\n\",\n\" <td>0.9313</td>\\n\",\n\" <td>0.9338</td>\\n\",\n\" <td>0.9580</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>9</th>\\n\",\n\" <td>C:1-linear-auto</td>\\n\",\n\" <td>0.9338</td>\\n\",\n\" <td>0.9313</td>\\n\",\n\" <td>0.9338</td>\\n\",\n\" <td>0.9580</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>14</th>\\n\",\n\" <td>C:1-rbf-scale</td>\\n\",\n\" <td>0.9247</td>\\n\",\n\" <td>0.9271</td>\\n\",\n\" <td>0.9247</td>\\n\",\n\" <td>0.8845</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>16</th>\\n\",\n\" <td>C:1-sigmoid-scale</td>\\n\",\n\" <td>0.9273</td>\\n\",\n\" <td>0.9245</td>\\n\",\n\" <td>0.9273</td>\\n\",\n\" <td>0.9525</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>6</th>\\n\",\n\" <td>C:0.5-rbf-scale</td>\\n\",\n\" <td>0.9228</td>\\n\",\n\" <td>0.9196</td>\\n\",\n\" <td>0.9228</td>\\n\",\n\" <td>0.9505</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>24</th>\\n\",\n\" <td>C:5-sigmoid-scale</td>\\n\",\n\" <td>0.9198</td>\\n\",\n\" <td>0.9192</td>\\n\",\n\" <td>0.9198</td>\\n\",\n\" <td>0.9167</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>22</th>\\n\",\n\" <td>C:5-rbf-scale</td>\\n\",\n\" <td>0.9222</td>\\n\",\n\" <td>0.9190</td>\\n\",\n\" <td>0.9222</td>\\n\",\n\" <td>0.9497</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>1</th>\\n\",\n\" <td>C:0.5-linear-auto</td>\\n\",\n\" <td>0.9223</td>\\n\",\n\" <td>0.9184</td>\\n\",\n\" <td>0.9223</td>\\n\",\n\" <td>0.9584</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>2</th>\\n\",\n\" <td>C:0.5-linear-scale</td>\\n\",\n\" <td>0.9223</td>\\n\",\n\" <td>0.9184</td>\\n\",\n\" <td>0.9223</td>\\n\",\n\" <td>0.9584</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>8</th>\\n\",\n\" <td>C:0.5-sigmoid-scale</td>\\n\",\n\" <td>0.9166</td>\\n\",\n\" <td>0.9122</td>\\n\",\n\" <td>0.9166</td>\\n\",\n\" <td>0.9544</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>23</th>\\n\",\n\" <td>C:5-sigmoid-auto</td>\\n\",\n\" <td>0.8973</td>\\n\",\n\" <td>0.8872</td>\\n\",\n\" <td>0.8973</td>\\n\",\n\" <td>0.9749</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>21</th>\\n\",\n\" <td>C:5-rbf-auto</td>\\n\",\n\" <td>0.8080</td>\\n\",\n\" <td>0.7619</td>\\n\",\n\" <td>0.8080</td>\\n\",\n\" <td>0.9910</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>7</th>\\n\",\n\" <td>C:0.5-sigmoid-auto</td>\\n\",\n\" <td>0.7701</td>\\n\",\n\" <td>0.7004</td>\\n\",\n\" <td>0.7701</td>\\n\",\n\" <td>0.9925</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>15</th>\\n\",\n\" <td>C:1-sigmoid-auto</td>\\n\",\n\" <td>0.7701</td>\\n\",\n\" <td>0.7004</td>\\n\",\n\" <td>0.7701</td>\\n\",\n\" <td>0.9925</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>13</th>\\n\",\n\" <td>C:1-rbf-auto</td>\\n\",\n\" <td>0.7673</td>\\n\",\n\" <td>0.6946</td>\\n\",\n\" <td>0.7673</td>\\n\",\n\" <td>0.9957</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>5</th>\\n\",\n\" <td>C:0.5-rbf-auto</td>\\n\",\n\" <td>0.7673</td>\\n\",\n\" <td>0.6946</td>\\n\",\n\" <td>0.7673</td>\\n\",\n\" <td>0.9957</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>20</th>\\n\",\n\" <td>C:5-poly-scale</td>\\n\",\n\" <td>0.7173</td>\\n\",\n\" <td>0.6108</td>\\n\",\n\" <td>0.7173</td>\\n\",\n\" <td>0.9768</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>19</th>\\n\",\n\" <td>C:5-poly-auto</td>\\n\",\n\" <td>0.6889</td>\\n\",\n\" <td>0.5473</td>\\n\",\n\" <td>0.6889</td>\\n\",\n\" <td>0.9914</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>11</th>\\n\",\n\" <td>C:1-poly-auto</td>\\n\",\n\" <td>0.6888</td>\\n\",\n\" <td>0.5469</td>\\n\",\n\" <td>0.6888</td>\\n\",\n\" <td>0.9918</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>12</th>\\n\",\n\" <td>C:1-poly-scale</td>\\n\",\n\" <td>0.6846</td>\\n\",\n\" <td>0.5357</td>\\n\",\n\" <td>0.6846</td>\\n\",\n\" <td>0.9965</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>3</th>\\n\",\n\" <td>C:0.5-poly-auto</td>\\n\",\n\" <td>0.6806</td>\\n\",\n\" <td>0.5258</td>\\n\",\n\" <td>0.6806</td>\\n\",\n\" <td>0.9992</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>4</th>\\n\",\n\" <td>C:0.5-poly-scale</td>\\n\",\n\" <td>0.0848</td>\\n\",\n\" <td>0.5157</td>\\n\",\n\" <td>0.0848</td>\\n\",\n\" <td>0.0444</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>0</th>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0.0000</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" </tr>\\n\",\n\" </tbody>\\n\",\n\"</table>\\n\",\n\"</div>\"\n],\n\"text/plain\": [\n\" Method F1 ACC Precision Recall\\n\",\n\"18 C:5-linear-scale 0.9351 0.9335 0.9351 0.9485\\n\",\n\"17 C:5-linear-auto 0.9351 0.9335 0.9351 0.9485\\n\",\n\"10 C:1-linear-scale 0.9338 0.9313 0.9338 0.9580\\n\",\n\"9 C:1-linear-auto 0.9338 0.9313 0.9338 0.9580\\n\",\n\"14 C:1-rbf-scale 0.9247 0.9271 0.9247 0.8845\\n\",\n\"16 C:1-sigmoid-scale 0.9273 0.9245 0.9273 0.9525\\n\",\n\"6 C:0.5-rbf-scale 0.9228 0.9196 0.9228 0.9505\\n\",\n\"24 C:5-sigmoid-scale 0.9198 0.9192 0.9198 0.9167\\n\",\n\"22 C:5-rbf-scale 0.9222 0.9190 0.9222 0.9497\\n\",\n\"1 C:0.5-linear-auto 0.9223 0.9184 0.9223 0.9584\\n\",\n\"2 C:0.5-linear-scale 0.9223 0.9184 0.9223 0.9584\\n\",\n\"8 C:0.5-sigmoid-scale 0.9166 0.9122 0.9166 0.9544\\n\",\n\"23 C:5-sigmoid-auto 0.8973 0.8872 0.8973 0.9749\\n\",\n\"21 C:5-rbf-auto 0.8080 0.7619 0.8080 0.9910\\n\",\n\"7 C:0.5-sigmoid-auto 0.7701 0.7004 0.7701 0.9925\\n\",\n\"15 C:1-sigmoid-auto 0.7701 0.7004 0.7701 0.9925\\n\",\n\"13 C:1-rbf-auto 0.7673 0.6946 0.7673 0.9957\\n\",\n\"5 C:0.5-rbf-auto 0.7673 0.6946 0.7673 0.9957\\n\",\n\"20 C:5-poly-scale 0.7173 0.6108 0.7173 0.9768\\n\",\n\"19 C:5-poly-auto 0.6889 0.5473 0.6889 0.9914\\n\",\n\"11 C:1-poly-auto 0.6888 0.5469 0.6888 0.9918\\n\",\n\"12 C:1-poly-scale 0.6846 0.5357 0.6846 0.9965\\n\",\n\"3 C:0.5-poly-auto 0.6806 0.5258 0.6806 0.9992\\n\",\n\"4 C:0.5-poly-scale 0.0848 0.5157 0.0848 0.0444\\n\",\n\"0 0 0 0.0000 0 0\"\n]\n},\n\"execution_count\": 25,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"result.sort_values('ACC',ascending=False)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 18,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/plain\": [\n\" C_CONTIGUOUS : True\\n\",\n\" F_CONTIGUOUS : False\\n\",\n\" OWNDATA : True\\n\",\n\" WRITEABLE : True\\n\",\n\" ALIGNED : True\\n\",\n\" WRITEBACKIFCOPY : False\\n\",\n\" UPDATEIFCOPY : False\"\n]\n},\n\"execution_count\": 18,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"X_train.flags\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 44,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"#存放结果\\n\",\n\"array_0 = np.zeros((1,5))\\n\",\n\"colums = ['Method','F1', 'ACC','Precision','Recall']\\n\",\n\"result = pd.DataFrame(array_0, index=[0], columns=colums)\\n\",\n\"methods = ['GaussianNB', 'MultinomialNB','ComplementNB','BernoulliNB']\\n\",\n\"for i,method in enumerate(methods): \\n\",\n\" if i==2:\\n\",\n\" new_result = Navie_baye(X_train, X_test, y_train, y_test,method=i,norm=True)\\n\",\n\" result.loc[len(result)] = [method+' norm: True']+new_result\\n\",\n\" new_result = Navie_baye(X_train, X_test, y_train, y_test,method=i,norm=False)\\n\",\n\" result.loc[len(result)] = [method+' norm: False']+new_result\\n\",\n\" else:\\n\",\n\" new_result = Navie_baye(X_train, X_test, y_train, y_test,method=i)\\n\",\n\" result.loc[len(result)] = [method]+new_result\\n\",\n\"#result.to_sql('de_tree_0', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 49,\n\"metadata\": {},\n\"outputs\": [\n{\n\"name\": \"stderr\",\n\"output_type\": \"stream\",\n\"text\": [\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\sql.py:1336: UserWarning: The provided table name 'Navie_baye_0' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\\n\",\n\" warnings.warn(msg, UserWarning)\\n\"\n]\n}\n],\n\"source\": [\n\"result.to_sql('Navie_baye_0', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 46,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/html\": [\n\"<div>\\n\",\n\"<style scoped>\\n\",\n\" .dataframe tbody tr th:only-of-type {\\n\",\n\" vertical-align: middle;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe tbody tr th {\\n\",\n\" vertical-align: top;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe thead th {\\n\",\n\" text-align: right;\\n\",\n\" }\\n\",\n\"</style>\\n\",\n\"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n\" <thead>\\n\",\n\" <tr style=\\\"text-align: right;\\\">\\n\",\n\" <th></th>\\n\",\n\" <th>Method</th>\\n\",\n\" <th>F1</th>\\n\",\n\" <th>ACC</th>\\n\",\n\" <th>Precision</th>\\n\",\n\" <th>Recall</th>\\n\",\n\" </tr>\\n\",\n\" </thead>\\n\",\n\" <tbody>\\n\",\n\" <tr>\\n\",\n\" <th>0</th>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>1</th>\\n\",\n\" <td>GaussianNB</td>\\n\",\n\" <td>0.9176</td>\\n\",\n\" <td>0.9170</td>\\n\",\n\" <td>0.9176</td>\\n\",\n\" <td>0.9144</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>2</th>\\n\",\n\" <td>MultinomialNB</td>\\n\",\n\" <td>0.8779</td>\\n\",\n\" <td>0.8824</td>\\n\",\n\" <td>0.8779</td>\\n\",\n\" <td>0.8362</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>3</th>\\n\",\n\" <td>ComplementNB norm: True</td>\\n\",\n\" <td>0.8839</td>\\n\",\n\" <td>0.8824</td>\\n\",\n\" <td>0.8839</td>\\n\",\n\" <td>0.8849</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>4</th>\\n\",\n\" <td>ComplementNB norm: False</td>\\n\",\n\" <td>0.8782</td>\\n\",\n\" <td>0.8826</td>\\n\",\n\" <td>0.8782</td>\\n\",\n\" <td>0.8370</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>5</th>\\n\",\n\" <td>BernoulliNB</td>\\n\",\n\" <td>0.8285</td>\\n\",\n\" <td>0.8195</td>\\n\",\n\" <td>0.8285</td>\\n\",\n\" <td>0.8621</td>\\n\",\n\" </tr>\\n\",\n\" </tbody>\\n\",\n\"</table>\\n\",\n\"</div>\"\n],\n\"text/plain\": [\n\" Method F1 ACC Precision Recall\\n\",\n\"0 0 0 0 0 0\\n\",\n\"1 GaussianNB 0.9176 0.9170 0.9176 0.9144\\n\",\n\"2 MultinomialNB 0.8779 0.8824 0.8779 0.8362\\n\",\n\"3 ComplementNB norm: True 0.8839 0.8824 0.8839 0.8849\\n\",\n\"4 ComplementNB norm: False 0.8782 0.8826 0.8782 0.8370\\n\",\n\"5 BernoulliNB 0.8285 0.8195 0.8285 0.8621\"\n]\n},\n\"execution_count\": 46,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"result\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 154,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"#存放结果\\n\",\n\"array_0 = np.zeros((1,5))\\n\",\n\"colums = ['Method','F1', 'ACC','Precision','Recall']\\n\",\n\"result = pd.DataFrame(array_0, index=[0], columns=colums)\\n\",\n\"#KNN模型测试\\n\",\n\"for method in ['gini', 'entropy']: \\n\",\n\" for depth in range(100,140,10):\\n\",\n\" new_result = decision_tree(X_train, X_test, y_train, y_test,criterion=method,max_depth=depth)\\n\",\n\" result.loc[len(result)] = [method+' max_dep:'+str(depth)]+new_result\\n\",\n\"#result.to_sql('de_tree_0', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": null,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"result.to_sql('de_tree_0', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 45,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/html\": [\n\"<div>\\n\",\n\"<style scoped>\\n\",\n\" .dataframe tbody tr th:only-of-type {\\n\",\n\" vertical-align: middle;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe tbody tr th {\\n\",\n\" vertical-align: top;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe thead th {\\n\",\n\" text-align: right;\\n\",\n\" }\\n\",\n\"</style>\\n\",\n\"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n\" <thead>\\n\",\n\" <tr style=\\\"text-align: right;\\\">\\n\",\n\" <th></th>\\n\",\n\" <th>Method</th>\\n\",\n\" <th>F1</th>\\n\",\n\" <th>ACC</th>\\n\",\n\" <th>Precision</th>\\n\",\n\" <th>Recall</th>\\n\",\n\" </tr>\\n\",\n\" </thead>\\n\",\n\" <tbody>\\n\",\n\" <tr>\\n\",\n\" <th>0</th>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>1</th>\\n\",\n\" <td>GaussianNB</td>\\n\",\n\" <td>0.9176</td>\\n\",\n\" <td>0.9170</td>\\n\",\n\" <td>0.9176</td>\\n\",\n\" <td>0.9144</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>2</th>\\n\",\n\" <td>MultinomialNB</td>\\n\",\n\" <td>0.8779</td>\\n\",\n\" <td>0.8824</td>\\n\",\n\" <td>0.8779</td>\\n\",\n\" <td>0.8362</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>3</th>\\n\",\n\" <td>ComplementNB norm: True</td>\\n\",\n\" <td>0.8839</td>\\n\",\n\" <td>0.8824</td>\\n\",\n\" <td>0.8839</td>\\n\",\n\" <td>0.8849</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>4</th>\\n\",\n\" <td>ComplementNB norm: False</td>\\n\",\n\" <td>0.8782</td>\\n\",\n\" <td>0.8826</td>\\n\",\n\" <td>0.8782</td>\\n\",\n\" <td>0.8370</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>5</th>\\n\",\n\" <td>BernoulliNB</td>\\n\",\n\" <td>0.8285</td>\\n\",\n\" <td>0.8195</td>\\n\",\n\" <td>0.8285</td>\\n\",\n\" <td>0.8621</td>\\n\",\n\" </tr>\\n\",\n\" </tbody>\\n\",\n\"</table>\\n\",\n\"</div>\"\n],\n\"text/plain\": [\n\" Method F1 ACC Precision Recall\\n\",\n\"0 0 0 0 0 0\\n\",\n\"1 GaussianNB 0.9176 0.9170 0.9176 0.9144\\n\",\n\"2 MultinomialNB 0.8779 0.8824 0.8779 0.8362\\n\",\n\"3 ComplementNB norm: True 0.8839 0.8824 0.8839 0.8849\\n\",\n\"4 ComplementNB norm: False 0.8782 0.8826 0.8782 0.8370\\n\",\n\"5 BernoulliNB 0.8285 0.8195 0.8285 0.8621\"\n]\n},\n\"execution_count\": 45,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"result\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 98,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"#存放结果\\n\",\n\"array_0 = np.zeros((1,5))\\n\",\n\"colums = ['Method','F1', 'ACC','Precision','Recall']\\n\",\n\"result = pd.DataFrame(array_0, index=[0], columns=colums)\\n\",\n\"#KNN模型测试\\n\",\n\"methods = ['cosine', 'euclidean', 'minkowski']\\n\",\n\"for method in methods: \\n\",\n\" new_result = KNN(X_train, X_test, y_train, y_test,method)\\n\",\n\" result.loc[len(result)] = [method]+new_result\\n\",\n\" \\n\",\n\"#result.to_sql('lo_re_1', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 86,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/html\": [\n\"<div>\\n\",\n\"<style scoped>\\n\",\n\" .dataframe tbody tr th:only-of-type {\\n\",\n\" vertical-align: middle;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe tbody tr th {\\n\",\n\" vertical-align: top;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe thead th {\\n\",\n\" text-align: right;\\n\",\n\" }\\n\",\n\"</style>\\n\",\n\"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n\" <thead>\\n\",\n\" <tr style=\\\"text-align: right;\\\">\\n\",\n\" <th></th>\\n\",\n\" <th>Method</th>\\n\",\n\" <th>F1</th>\\n\",\n\" <th>ACC</th>\\n\",\n\" <th>Precision</th>\\n\",\n\" <th>Recall</th>\\n\",\n\" </tr>\\n\",\n\" </thead>\\n\",\n\" <tbody>\\n\",\n\" <tr>\\n\",\n\" <th>0</th>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>1</th>\\n\",\n\" <td>cosine</td>\\n\",\n\" <td>0.8450</td>\\n\",\n\" <td>0.8485</td>\\n\",\n\" <td>0.8450</td>\\n\",\n\" <td>0.8401</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>2</th>\\n\",\n\" <td>euclidean</td>\\n\",\n\" <td>0.7284</td>\\n\",\n\" <td>0.6984</td>\\n\",\n\" <td>0.7284</td>\\n\",\n\" <td>0.8227</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>3</th>\\n\",\n\" <td>minkowski</td>\\n\",\n\" <td>0.7284</td>\\n\",\n\" <td>0.6984</td>\\n\",\n\" <td>0.7284</td>\\n\",\n\" <td>0.8227</td>\\n\",\n\" </tr>\\n\",\n\" </tbody>\\n\",\n\"</table>\\n\",\n\"</div>\"\n],\n\"text/plain\": [\n\" Method F1 ACC Precision Recall\\n\",\n\"0 0 0 0 0 0\\n\",\n\"1 cosine 0.8450 0.8485 0.8450 0.8401\\n\",\n\"2 euclidean 0.7284 0.6984 0.7284 0.8227\\n\",\n\"3 minkowski 0.7284 0.6984 0.7284 0.8227\"\n]\n},\n\"execution_count\": 86,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"result #用cosine度量方式模型准确率要高\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 145,\n\"metadata\": {},\n\"outputs\": [\n{\n\"name\": \"stderr\",\n\"output_type\": \"stream\",\n\"text\": [\n\"D:\\\\Design Software\\\\Anaconda\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\sklearn\\\\linear_model\\\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\n\",\n\" \\\"the coef_ did not converge\\\", ConvergenceWarning)\\n\"\n]\n}\n],\n\"source\": [\n\"#存放结果\\n\",\n\"array_0 = np.zeros((1,5))\\n\",\n\"colums = ['Method','F1', 'ACC','Precision','Recall']\\n\",\n\"result = pd.DataFrame(array_0, index=[0], columns=colums)\\n\",\n\"solvers = ['newton-cg','lbfgs', 'liblinear', 'sag', 'saga']\\n\",\n\"for penalty in ['l1','l2','elasticnet','none']:\\n\",\n\" try:\\n\",\n\" if penalty == 'elasticnet':\\n\",\n\" for i in [0.1, 0.2 ,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\\n\",\n\" new_result = logical_regression(X_train, X_test, y_train, y_test, \\n\",\n\" penalty=penalty,l1_ratio=i, solver='saga',C=5)\\n\",\n\" result.loc[len(result)]= ['l1:'+str(i)+'l2:'+str(1-i)]+new_result\\n\",\n\" else:\\n\",\n\" new_result = logical_regression(X_train, X_test, y_train, y_test, penalty=penalty)\\n\",\n\" result.loc[len(result)]= [penalty]+new_result\\n\",\n\" except:\\n\",\n\" pass\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 141,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/plain\": [\n\"array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\"\n]\n},\n\"execution_count\": 141,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"np.linspace(0.1, 0.9, 9)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 146,\n\"metadata\": {},\n\"outputs\": [\n{\n\"data\": {\n\"text/html\": [\n\"<div>\\n\",\n\"<style scoped>\\n\",\n\" .dataframe tbody tr th:only-of-type {\\n\",\n\" vertical-align: middle;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe tbody tr th {\\n\",\n\" vertical-align: top;\\n\",\n\" }\\n\",\n\"\\n\",\n\" .dataframe thead th {\\n\",\n\" text-align: right;\\n\",\n\" }\\n\",\n\"</style>\\n\",\n\"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n\" <thead>\\n\",\n\" <tr style=\\\"text-align: right;\\\">\\n\",\n\" <th></th>\\n\",\n\" <th>Method</th>\\n\",\n\" <th>F1</th>\\n\",\n\" <th>ACC</th>\\n\",\n\" <th>Precision</th>\\n\",\n\" <th>Recall</th>\\n\",\n\" </tr>\\n\",\n\" </thead>\\n\",\n\" <tbody>\\n\",\n\" <tr>\\n\",\n\" <th>0</th>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" <td>0</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>1</th>\\n\",\n\" <td>l1</td>\\n\",\n\" <td>0.9686</td>\\n\",\n\" <td>0.9694</td>\\n\",\n\" <td>0.9686</td>\\n\",\n\" <td>0.9592</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>2</th>\\n\",\n\" <td>l2</td>\\n\",\n\" <td>0.9443</td>\\n\",\n\" <td>0.9476</td>\\n\",\n\" <td>0.9443</td>\\n\",\n\" <td>0.9043</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>3</th>\\n\",\n\" <td>l1:0.1l2:0.9</td>\\n\",\n\" <td>0.9652</td>\\n\",\n\" <td>0.9666</td>\\n\",\n\" <td>0.9652</td>\\n\",\n\" <td>0.9418</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>4</th>\\n\",\n\" <td>l1:0.2l2:0.8</td>\\n\",\n\" <td>0.9663</td>\\n\",\n\" <td>0.9676</td>\\n\",\n\" <td>0.9663</td>\\n\",\n\" <td>0.9447</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>5</th>\\n\",\n\" <td>l1:0.3l2:0.7</td>\\n\",\n\" <td>0.9668</td>\\n\",\n\" <td>0.9680</td>\\n\",\n\" <td>0.9668</td>\\n\",\n\" <td>0.9455</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>6</th>\\n\",\n\" <td>l1:0.4l2:0.6</td>\\n\",\n\" <td>0.9678</td>\\n\",\n\" <td>0.9690</td>\\n\",\n\" <td>0.9678</td>\\n\",\n\" <td>0.9483</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>7</th>\\n\",\n\" <td>l1:0.5l2:0.5</td>\\n\",\n\" <td>0.9674</td>\\n\",\n\" <td>0.9686</td>\\n\",\n\" <td>0.9674</td>\\n\",\n\" <td>0.9483</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>8</th>\\n\",\n\" <td>l1:0.6l2:0.4</td>\\n\",\n\" <td>0.9693</td>\\n\",\n\" <td>0.9704</td>\\n\",\n\" <td>0.9693</td>\\n\",\n\" <td>0.9515</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>9</th>\\n\",\n\" <td>l1:0.7l2:0.30000000000000004</td>\\n\",\n\" <td>0.9713</td>\\n\",\n\" <td>0.9722</td>\\n\",\n\" <td>0.9713</td>\\n\",\n\" <td>0.9552</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>10</th>\\n\",\n\" <td>l1:0.8l2:0.19999999999999996</td>\\n\",\n\" <td>0.9727</td>\\n\",\n\" <td>0.9736</td>\\n\",\n\" <td>0.9727</td>\\n\",\n\" <td>0.9580</td>\\n\",\n\" </tr>\\n\",\n\" <tr>\\n\",\n\" <th>11</th>\\n\",\n\" <td>l1:0.9l2:0.09999999999999998</td>\\n\",\n\" <td>0.9740</td>\\n\",\n\" <td>0.9748</td>\\n\",\n\" <td>0.9740</td>\\n\",\n\" <td>0.9608</td>\\n\",\n\" </tr>\\n\",\n\" </tbody>\\n\",\n\"</table>\\n\",\n\"</div>\"\n],\n\"text/plain\": [\n\" Method F1 ACC Precision Recall\\n\",\n\"0 0 0 0 0 0\\n\",\n\"1 l1 0.9686 0.9694 0.9686 0.9592\\n\",\n\"2 l2 0.9443 0.9476 0.9443 0.9043\\n\",\n\"3 l1:0.1l2:0.9 0.9652 0.9666 0.9652 0.9418\\n\",\n\"4 l1:0.2l2:0.8 0.9663 0.9676 0.9663 0.9447\\n\",\n\"5 l1:0.3l2:0.7 0.9668 0.9680 0.9668 0.9455\\n\",\n\"6 l1:0.4l2:0.6 0.9678 0.9690 0.9678 0.9483\\n\",\n\"7 l1:0.5l2:0.5 0.9674 0.9686 0.9674 0.9483\\n\",\n\"8 l1:0.6l2:0.4 0.9693 0.9704 0.9693 0.9515\\n\",\n\"9 l1:0.7l2:0.30000000000000004 0.9713 0.9722 0.9713 0.9552\\n\",\n\"10 l1:0.8l2:0.19999999999999996 0.9727 0.9736 0.9727 0.9580\\n\",\n\"11 l1:0.9l2:0.09999999999999998 0.9740 0.9748 0.9740 0.9608\"\n]\n},\n\"execution_count\": 146,\n\"metadata\": {},\n\"output_type\": \"execute_result\"\n}\n],\n\"source\": [\n\"result\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 87,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"from sqlalchemy import create_engine\\n\",\n\"engine = create_engine(\\\"mysql://root:forever@127.0.0.1:3306/sklearn_result\\\", echo=False)\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 147,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"result.to_sql('lo_re_1', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 115,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"#存放结果\\n\",\n\"array_0 = np.zeros((1,5))\\n\",\n\"colums = ['Method','F1', 'ACC','Precision','Recall']\\n\",\n\"result = pd.DataFrame(array_0, index=[0], columns=colums)\\n\",\n\"for pene in np.linspace(1,10,10):\\n\",\n\" try: \\n\",\n\" new_result = logical_regression(X_train, X_test, y_train, y_test,C=c)\\n\",\n\" c = str(format(c,'.2f'))\\n\",\n\" result.loc[len(result)] = ['C='+c]+new_result\\n\",\n\" except:\\n\",\n\" pass\\n\",\n\"#result.to_sql('knn_1', con=engine ,if_exists='replace')\"\n]\n},\n{\n\"cell_type\": \"code\",\n\"execution_count\": 117,\n\"metadata\": {},\n\"outputs\": [],\n\"source\": [\n\"result.to_sql('lo_re_0', con=engine ,if_exists='replace')\"\n]\n}\n],\n\"metadata\": {\n\"kernelspec\": {\n\"display_name\": \"Python 3\",\n\"language\": \"python\",\n\"name\": \"python3\"\n},\n\"language_info\": {\n\"codemirror_mode\": {\n\"name\": \"ipython\",\n\"version\": 3\n},\n\"file_extension\": \".py\",\n\"mimetype\": \"text/x-python\",\n\"name\": \"python\",\n\"nbconvert_exporter\": \"python\",\n\"pygments_lexer\": \"ipython3\",\n\"version\": \"3.6.9\"\n},\n\"toc\": {\n\"base_numbering\": 1,\n\"nav_menu\": {},\n\"number_sections\": true,\n\"sideBar\": true,\n\"skip_h1_title\": false,\n\"title_cell\": \"Table of Contents\",\n\"title_sidebar\": \"Contents\",\n\"toc_cell\": false,\n\"toc_position\": {},\n\"toc_section_display\": true,\n\"toc_window_display\": false\n}\n},\n\"nbformat\": 4,\n\"nbformat_minor\": 2\n}\n下载|复制|删除当前路径：cells[0].source[4]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
